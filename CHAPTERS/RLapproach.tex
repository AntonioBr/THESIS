\chapter{A Reinforcement Learning Approach to Black-Box Optimization}

One of the principal challenges in optimization practice is how to optimize in absence of an algebraic model of the system to be optimized. This kind of optimization is known as \textit{black-box optimization}. \\

A black-box function $f(x) : \mathbb{R}^n \rightarrow \mathbb{R}$ is a function for which the analytic form is not known. Nowadays there are lots of mathematical models to solve these type of functions. One of the best known of these methods is the \textit{Bayesian Optimization Model} (BO). \\
In this chapter we will first explain how BO works and than we will propose an innovative, RL based approach for black-box optimization problem.

\paragraph{Gaussian Processes} Before starting to speak about BO we have to describe what \textit{Gaussian Processes} (GPs) are. GPs are an alternative approach to regression problems. The GP approach is a \textit{non-parametric} (we don't have a priori knowledge of how many parameters will be useful for our regression) approach to find a distribution over the possible function $f(x)$ that are consistent with observed data. A GP is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions.
GP is a convenient and powerful prior distribution on functions, which we will take here to be of the form $f : \mathcal{X} \leftarrow \mathbb{R}$. The GP is defined by the property that any finite set of $N$ points $\{x_n \in \mathcal{X}\}\subsup{}{ n=1}{N}$ induces a multivariate Gaussian distribution  on $\mathbb{R}^N$. The $n$th of these points is taken to be the function value $f(x_n)$. The support and properties of the resulting distribution on functions are determined by a mean function $m : \mathcal{X} \leftarrow \mathbb{R}$ and a positive definite covariance function $K : \mathcal{X} \times \mathcal{X} \leftarrow \mathbb{R}$ ~\cite{NIPS2012_4522}.

\begin{figure} [h!]
	\centering
	\includegraphics[width= \textwidth, height = 9.5cm]{Multivariate_Gaussian.png}
	\caption{Multivariate Gaussian Distribution [Wikipedia]}
	\label{fig:Multivatiate_Gaussian}
\end{figure}

\paragraph{Acquisition Functions for Bayesian Optimization} Let's assume that the function $f(x)$ is drawn from a GP prior and that our observation are of the form $\{x_n \in \mathcal{X}\}\subsup{}{ n=1}{N}$, where $y_n \sim \mathcal{N}(f(x_n), v)$ and $v$ is the variance of noise introduced into the function observations. This prior and these data induce  posterior over functions; the acquisition function, which we denote by $a : \mathcal{X} \leftarrow \mathbb{R}^+$, determines what point in $\mathcal{X}$ should be evaluated next via a proxy optimization $x\textsubscript{next} = \arg\max_{x}a(x)$, where several different functions have been proposed. There are several popular choices of acquisition function. Under the Gaussian process prior, these functions depend on the model solely through its predictive mean function $\mu(x; \{x_n, y_n\})$ and predictive variance function $\sigma^2(x; \{x_n, y_n\})$~\cite{NIPS2012_4522}.

\begin{figure} [h!]
	\centering
	\includegraphics[width= \textwidth, height = 13cm]{BOProcess.png}
	\caption{2-$d$ Bayesian Optimization Process Example[towardsdatascience.com]}
	\label{fig:BoProcess}
\end{figure}

\paragraph{An RL Approach} As previously said in this thesis we describe an innovative RL approach to the black-box function optimization problem. Let's assume to dispose of a function $f(x, y)$. In our RL problem :

\begin{itemize}
	\item \textbf{Agent} : The agent has to maximize a black-box bivariate function. Each function is continuously defined over a specific domain. The agent has to complete its job making exactly $150$ epochs for each one of the $1000$ episodes. In each epoch it has a position in space described through the two coordinates $(x, y)$. Each time the agent makes an action the angle between $(x, y)$ and $(x', y')$ and the value of the function $f(x', y')$ are computed.
	\item \textbf{State} : The state is represented by two lists: the first one contains the last two computed \textit{angles} and the second one contains the correspondent last two \textit{actions}.
	\item \textbf{Actions} : In each epoch the agent can make one of four different actions : \textit{move north}, \textit{move  south}, \textit{move east}, \textit{move west}. Each time the agent moves itself of $40$ pixels in one of the previously described directions. The resultant effective movement is computed as follow:
	
	\begin{algorithm} [h!]
		/* knowing $pixelX$ and $pixelY$ */\;
		/* knowing $pixelXRange$ and $pixelYRange$ */ \;
		/* knowing $function$ */\;
		
		\
		
		$domain = function.getDomain()$ \;
		$xRange = domain.maxX - domain.minX$ \;
		$yRange = domain.maxY - domain.minY$ \;
		
		\
		
		$xReal = domain.minX + (pixelX * xRange) / pixelXRange$ \;
		$yReal = domain.minY + (pixelY * yRange) / pixelYRange$ \;
		
		\
		
		\KwRet{$xReal, yReal$}
		\caption{From pixels to real values} 
	\end{algorithm}
	
	\item \textbf{Reward} : In this context we have decided to reward every action of the agent because a real terminal state doesn't exist. Cause we are working with a black-box function we cannot select two coordinate $(x, y)$ and a corresponding value function $z$ as a terminal state. The simulation ends after $1000$ episodes are done. We define a $\Delta$ equals to $\max f(z_n)$ minus $f(z)$ computed in the \textit{current state}:
	
	\begin{equation}
		\Delta = max f(x_n, y_n) - f(x, y)
	\end{equation} 
	
	Our reward at each epoch is equal to $\Delta$.
\end{itemize}

The movement the agent can make in each epoch can be of two different types : \textit{linear movement} or \textit{parametric movement}. If the movement is linear we compute the angle as follow :

\begin{algorithm}
	/* knowing $(x, y)$ */ \;
	/* knowing $(x', y')$*/ \;
	/* {\tt movementAmount} = \textit{M} */ \;
	/* {\tt currentMax} = $\max f(x_n, y_n)$ */ \;
	
	
	\
	
	$z = f(x, y)$ \;
	$z' = f(x', y')$\;
	
	\
	
	$\delta = ((x'-x),  (y'-y))$ \;
	$\alpha = \arctan(\dfrac{\delta}{\tt movementAmount})$
	 
	 \
	 
	 $\Delta = $z'$ - {\tt currentMax} $
	 
	 \caption{Angle computation in linear movement case.} 
	
\end{algorithm}

We can explain this algorithm briefly recalling some trigonometry. Let's suppose to be in a simple 2d case like the one represented in figure ~\ref{fig:LMComputations}.

\begin{figure} [h!]
	\centering
	\includegraphics[width= 7cm, height = 7cm]{Triangolo.png}
	\caption{Linear movement computations.}
	\label{fig:LMComputations}
\end{figure}

Let's suppose that the starting point of our RL agent at epoch \textit{e} is the point \textit{A} with coordinates $(x, y)$. In the represented case the agent can make only one of two actions for each epoch : \textit{move on} and \textit{go back}. Making a linear movement of the type \textit{move on} the arriving point at epoch \textit{e'} is \textit{B} with coordinates $(x', y)$. Knowing that the objective function is $f(x) = \sin(2x)$, we can now compute $f(x')$. We now know point \textit{C} with coordinates $(x', f(x'))$. \textit{u} is the {\tt movement amount} and \textit{CB} equals to $f(x') - f(x)$ is $\delta$. From trigonometry we know that 

\begin{equation}
	\tan \alpha = \dfrac{\delta}{\tt movementAmount},
\end{equation}

so 

\begin{equation}
	\alpha = \arctan \dfrac{\delta}{movementAmount}
\end{equation}

If the movement is parametric we want to better approximate the amount of movement over the function. We do this using the following algorithm :

\begin{algorithm}
	/* knowing $(x, y)$ */ \;
	/* knowing $(x', y')$*/ \;
	/* {\tt movementAmount} = \textit{M} */ \;
	/* {\tt currentMax} = $\max f(x_n, y_n)$ */ \;
	
	
	\
	
	$z = f(x, y)$ \;
	$z' = f(x', y')$\;
	
	\
	
	$\delta = ((x'-x),  (y'-y))$ \;
	$\alpha = \arctan(\dfrac{\delta}{\tt movementAmount})$ \;
	
	\
	
	$hypotenuse = \dfrac{{\tt movementAmount}}{\cos \alpha}$ \;
	
	\
	
	$movementAmountApprox = \dfrac{{\tt movementAmount} * {\tt movementAmount}}{hypotenuse}$
	
	$\Delta = $z'$ - {\tt currentMax} $\;
	
	\caption{Angle computation in parametric movement case.} 
	\label{PMAlgo}
	
\end{algorithm}

We can easily explain this algorithm looking at figure ~\ref{fig:PMComputations}. 

\begin{figure} [h!]
	\centering
	\includegraphics[width= 9cm, height = 9cm]{TRIANGOLO2.png}
	\caption{Parametric movement computations.}
	\label{fig:PMComputations}
\end{figure}

As already previously done, let's suppose that the starting point of our RL agent at epoch \textit{e} is the point \textit{A} with coordinates $(x, y)$. Because of the two dimensions, even in this case the agent can make only one of two actions for each epoch : \textit{move on} and \textit{go back}. Making a linear movement of the type \textit{move on} the arriving point at epoch \textit{e'} is \textit{B} with coordinates $(x', y)$. Knowing that the objective function is $f(x) = \sin(2x)$, we can now compute $f(x')$. We now know point \textit{C} with coordinates $(x', f(x'))$. \textit{u} is the {\tt movement amount} and \textit{CB} equals to $f(x') - f(x)$ is $\delta$. From trigonometry we know $\alpha$ but we are interested in knowing the real amount of movement done by the agent on the function or, at least, its approximation. We want to know how much is $\overline{AB}$.  From the first theorem of Euclid we know that \textit{in a right-angled triangle, the square constructed on a cathetus is equivalent to the rectangle that has for dimensions the hypotenuse and the projection of the cathetus on the hypotenuse}. This means that in a right-angled triangle, the cathetus is proportional medium between the hypotenuse and its own projection on it. According to this we can write the following proportion :

\begin{equation}
	i : u =  u : AD.
\end{equation}

So :

\begin{equation}
	AD = \dfrac{u * u}{i},
\end{equation}

that is the same of :

\begin{equation}
	 movementAmountApprox = \dfrac{{\tt movementAmount} * {\tt movementAmount}}{hypotenuse}
\end{equation}

of algorithm ~\ref{PMAlgo}.

The real question regards why select one method instead of the other one. The answer at this question is simple. Adopting the parametric approach the optimization process is longer but more accurate. In addition to this using the parametric approach we could ideally train our agent on a set of specific functions and it should do very good on a generic function using its knowledge about angles, actions and slope. 

Adopting the linear approach the optimization process is slower but less accurate. In addition to this using the linear approach we couldn't abstract from the concept of function.  This means that for every specific function we have to train our agent one more time.

\subsection{Implementation}

In order to formalize and solve our RL problem, we used the BURLAP (Brown-UMBC Reinforcement Learning and Planning) Java library developed and maintained by James MacGlashan. BURLAP uses a highly flexible system for defining states and and actions of nearly any kind of form, supporting discrete continuous, and relational domains. Planning and learning algorithms range from classic forward search planning to value function-based stochastic planning and learning algorithms.

In order to define specific MDPs, BURLAP offers a set of classes and interfaces (figure ~\ref{fig:UMLBurlap}).

\begin{figure} [h!]
	\includegraphics[width= 16cm, height = 9cm]{UMLBurlap.png}
	\caption{UML Digram of the Java interfaces/classes for an MDP definition.}
	\label{fig:UMLBurlap}
\end{figure}

A complete documentation for this framework can be find at (...). 

Starting from this features offered by BURLAP, we have extended interfaces and implemented abstract classes in order to modelling the problem described above as shown in figure (..). In our implementation ...

\begin{figure} [h!]
	\includegraphics[width= 16cm, height = 18cm]{RLDiagram.png}
	\caption{Class diagram of RL black-box optimization tool.}
	\label{fig:RLDiagram}
\end{figure}