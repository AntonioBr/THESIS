\chapter{A Reinforcement Learning Approach to Black-Box Optimization}

One of the principal challenges in optimization practice is how to optimize in absence of an algebraic model of the system to be optimized. This kind of optimization is known as \textit{black-box optimization}. \\

A black-box function $f(x) : \mathbb{R}^n \rightarrow \mathbb{R}$ is a function for which the analytic form is not known. Nowadays there are lots of mathematical models to solve these type of functions. One of the best known of these methods is the \textit{Bayesian Optimization Model} (BO). \\
In this chapter we will first explain how BO works and than we will propose an innovative, RL based approach for black-box optimization problem.

\paragraph{Gaussian Processes} Before starting to speak about BO we have to describe what \textit{Gaussian Processes} (GPs) are. GPs are an alternative approach to regression problems. The GP approach is a \textit{non-parametric} (we don't have a priori knowledge of how many parameters will be useful for our regression) approach to find a distribution over the possible function $f(x)$ that are consistent with observed data. A GP is a generalization of the Gaussian probability distribution. Whereas a probability distribution describes random variables which are scalars or vectors (for multivariate distributions), a stochastic process governs the properties of functions.
GP is a convenient and powerful prior distribution on functions, which we will take here to be of the form $f : \mathcal{X} \leftarrow \mathbb{R}$. The GP is defined by the property that any finite set of $N$ points $\{x_n \in \mathcal{X}\}\subsup{}{ n=1}{N}$ induces a multivariate Gaussian distribution  on $\mathbb{R}^N$. The $n$th of these points is taken to be the function value $f(x_n)$. The support and properties of the resulting distribution on functions are determined by a mean function $m : \mathcal{X} \leftarrow \mathbb{R}$ and a positive definite covariance function $K : \mathcal{X} \times \mathcal{X} \leftarrow \mathbb{R}$ ~\cite{NIPS2012_4522}.

\begin{figure} [h!]
	\centering
	\includegraphics[width= \textwidth, height = 9.5cm]{Multivariate_Gaussian.png}
	\caption{Multivariate Gaussian Distribution [Wikipedia]}
	\label{fig:Multivatiate_Gaussian}
\end{figure}

\paragraph{Acquisition Functions for Bayesian Optimization} Let's assume that the function $f(x)$ is drawn from a GP prior and that our observation are of the form $\{x_n \in \mathcal{X}\}\subsup{}{ n=1}{N}$, where $y_n \sim \mathcal{N}(f(x_n), v)$ and $v$ is the variance of noise introduced into the function observations. This prior and these data induce  posterior over functions; the acquisition function, which we denote by $a : \mathcal{X} \leftarrow \mathbb{R}^+$, determines what point in $\mathcal{X}$ should be evaluated next via a proxy optimization $x\textsubscript{next} = \arg\max_{x}a(x)$, where several different functions have been proposed. There are several popular choices of acquisition function. Under the Gaussian process prior, these functions depend on the model solely through its predictive mean function $\mu(x; \{x_n, y_n\})$ and predictive variance function $\sigma^2(x; \{x_n, y_n\})$~\cite{NIPS2012_4522}.

\begin{figure} [h!]
	\centering
	\includegraphics[width= \textwidth, height = 13cm]{BOProcess.png}
	\caption{2-$d$ Bayesian Optimization Process Example[towardsdatascience.com]}
	\label{fig:BoProcess}
\end{figure}

\paragraph{An RL Approach} As previously said in this thesis we describe an innovative RL approach to the black-box function optimization problem. Let's assume to dispose of a function $f(x, y)$. In our RL problem :

\begin{itemize}
	\item \textbf{Agent} : The agent has to maximize a black-box bivariate function. Each function is continuously defined over a specific domain. The agent has to complete its job making exactly $150$ epochs for each one of the $1000$ episodes. In each epoch it has a position in space described through the two coordinates $(x, y)$. Each time the agent makes an action the angle between $(x, y)$ and $(x', y')$ and the value of the function $f(x', y')$ are computed.
	\item \textbf{State} : The state is represented by two lists: the first one contains the last two computed \textit{angles} and the second one contains the correspondent last two \textit{actions}.
	\item \textbf{Actions} : In each epoch the agent can make one of four different actions : \textit{move north}, \textit{move  south}, \textit{move east}, \textit{move west}. Each time the agent moves itself of $40$ pixels in one of the previously described directions. The resultant effective movement is computed as follow:
	
	\begin{algorithm} [h!]
		/* knowing $pixelX$ and $pixelY$ */\;
		/* knowing $pixelXRange$ and $pixelYRange$ */ \;
		/* knowing $function$ */\;
		
		\
		
		$domain = function.getDomain()$ \;
		$xRange = domain.maxX - domain.minX$ \;
		$yRange = domain.maxY - domain.minY$ \;
		
		\
		
		$xReal = domain.minX + (pixelX * xRange) / pixelXRange$ \;
		$yReal = domain.minY + (pixelY * yRange) / pixelYRange$ \;
		
		\
		
		\KwRet{$xReal, yReal$}
		\caption{From pixels to real values} 
	\end{algorithm}
	
	\item \textbf{Reward} : In this context we have decided to reward every action of the agent because a real terminal state doesn't exist. Cause we are working with a black-box function we cannot select two coordinate $(x, y)$ and a corresponding value function $z$ as a terminal state. The simulation ends after $1000$ episodes are done. We define a $\Delta$ equals to $f(z')$ computed in the \textit{new state} minus $f(z)$ computed in the \textit{old state}:
	
	\begin{equation}
		\Delta = f(z') - f(z)
	\end{equation} 
	
	Our reward at each epoch is equal to $\Delta$.
\end{itemize}

The movement the agent can make in each epoch can be of two different types : \textit{linear movement} or \textit{parametric movement}. If the movement is linear we compute the angle as follow :

\begin{algorithm}
	/* knowing $(x, y)$ */ \;
	/* knowing $(x', y')$*/ \;
	/* {\tt movementAmount} = $\theta$ */ \;
	/* {\tt currentMax} = $\max f(x_n, y_n)$ */ \;
	
	
	\
	
	$z = f(x, y)$ \;
	$z' = f(x', y')$\;
	
	\
	
	$\delta = z' - z$ \;
	$\beta = \arctan(\dfrac{\delta}{\tt movementAmount})$
	 
	 \
	 
	 $\Delta = $z'$ - {\tt currentMax} $
	
	
	
\end{algorithm}

\subsection{Implementation}

\begin{figure} [h!]
	\includegraphics[width= 16cm, height = 18cm]{RLDiagram.png}
	\caption{Class diagram of RL black-box optimization tool.}
	\label{fig:BoProcess}
\end{figure}