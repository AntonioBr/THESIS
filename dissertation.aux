\relax 
\citation{HillLieb01}
\citation{DBLP:journals/pieee/ShahriariSWAF16}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\citation{Adams2008GaussianPP}
\citation{DBLP:journals/pieee/ShahriariSWAF16}
\citation{RLDef1}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\citation{konstantopoulos2009markov}
\citation{Put94}
\citation{wiering2012reinforcement}
\citation{Nevmyvaka}
\citation{Powell}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Process}{3}}
\@writefile{toc}{\contentsline {paragraph}{Markov Decision Process}{3}}
\citation{wiering2012reinforcement}
\citation{Sigaud:2010:MDP:1841781}
\citation{wiering2012reinforcement}
\citation{SuttonBarto}
\@writefile{toc}{\contentsline {subparagraph}{States}{4}}
\@writefile{toc}{\contentsline {subparagraph}{Actions}{4}}
\@writefile{toc}{\contentsline {subparagraph}{Time Steps}{4}}
\@writefile{toc}{\contentsline {subparagraph}{Transition Probability}{4}}
\@writefile{toc}{\contentsline {subparagraph}{Reward Function}{4}}
\citation{Sigaud:2010:MDP:1841781}
\citation{Sigaud:2010:MDP:1841781}
\citation{wiering2012reinforcement}
\citation{Sigaud:2010:MDP:1841781}
\citation{Sigaud:2010:MDP:1841781}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Markov decision process\nobreakspace  {}\cite  {Sigaud:2010:MDP:1841781}.}}{5}}
\newlabel{fig:MDP}{{2.1}{5}}
\@writefile{toc}{\contentsline {subparagraph}{Policy}{5}}
\citation{Sigaud:2010:MDP:1841781}
\citation{Mitchell}
\citation{SuttonBarto}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Different policy families for MDPs\nobreakspace  {}\cite  {Sigaud:2010:MDP:1841781}}}{6}}
\newlabel{table:T1}{{2.1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Relationship between the different sets of policies\nobreakspace  {}\cite  {Sigaud:2010:MDP:1841781}.}}{6}}
\newlabel{fig:Policies_schema}{{2.2}{6}}
\citation{wiering2012reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Policy application schema\nobreakspace  {}\cite  {SuttonBarto}.}}{7}}
\newlabel{fig:Policy_application_schema}{{2.3}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Optimality : \textbf  {a)} finite horizon, \textbf  {b)} discounted, infinite horizon, \textbf  {c)} average reward}}{7}}
\@writefile{toc}{\contentsline {subparagraph}{Discount Factor}{7}}
\@writefile{toc}{\contentsline {subparagraph}{Bellman Equation}{8}}
\newlabel{eq:2.2}{{2.3}{8}}
\citation{wiering2012reinforcement}
\citation{wiering2012reinforcement}
\citation{wiering2012reinforcement}
\citation{RLDef1}
\newlabel{eq:2.5}{{2.5}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Solving MDP}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Main differences between model-based and model-free algorithms.}}{9}}
\@writefile{toc}{\contentsline {subparagraph}{Dynamic Programming}{9}}
\citation{LiMalik}
\citation{Sigaud:2010:MDP:1841781}
\citation{Sigaud:2010:MDP:1841781}
\citation{SuttonBarto}
\citation{Sigaud:2010:MDP:1841781}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The nature of Reinforcement Learning}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\citation{Nevmyvaka}
\citation{Powell}
\citation{Mitchell}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Elements of reinforcement learning}{11}}
\@writefile{toc}{\contentsline {paragraph}{}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces RL working schema.}}{12}}
\newlabel{fig:RLWS}{{2.5}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Finite Markov Decision Processes}{12}}
\@writefile{toc}{\contentsline {paragraph}{}{12}}
\citation{Sigaud:2010:MDP:1841781}
\citation{SuttonBarto}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}The Agent-Environment Interface}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\citation{Maia2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The agent-environment interaction in a MDP.}}{14}}
\newlabel{fig:MDPS}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Goals, Rewards, Returns and Episodes}{14}}
\@writefile{toc}{\contentsline {paragraph}{}{14}}
\citation{SuttonBarto}
\citation{SuttonBarto}
\@writefile{toc}{\contentsline {paragraph}{}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Schema of an agent-environment interaction broken into two episodes ($E$), each one broken into three epochs ($e$).}}{15}}
\newlabel{fig:MDPS}{{2.7}{15}}
\@writefile{toc}{\contentsline {paragraph}{}{15}}
\citation{SuttonBarto}
\@writefile{toc}{\contentsline {paragraph}{}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Policies and Value Functions}{16}}
\@writefile{toc}{\contentsline {paragraph}{}{16}}
\citation{SuttonBarto}
\citation{SuttonBarto}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Think of looking ahead from a state to its possible successor states. Each open circle represents a state and each solid circle represents a state-action pair. Starting from state \textit  {s}, the root node at the top, the agent could take any of some set of actions - two are shown in the diagram - based on its policy $\pi $. From each of these, the environment could respond with one of several next states, $s'$, along with a reward, \textit  {r}, depending on its dynamics given by the function \textit  {p}. The Bellman equation (1.8) averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the discounted value of the expected next state, plus the reward expected along the way.}}{18}}
\newlabel{fig:Bellman_graph}{{2.8}{18}}
\bibdata{bibfile.bib}
\bibcite{Adams2008GaussianPP}{AS08}
\bibcite{HillLieb01}{HL01}
\bibcite{RLDef1}{KLM96}
\bibcite{konstantopoulos2009markov}{Kon09}
\bibcite{LiMalik}{LM16}
\bibcite{Maia2009}{Mai09}
\bibcite{Mitchell}{Mit97}
\bibcite{Nevmyvaka}{NFK06}
\bibcite{Powell}{Pow07}
\bibcite{Put94}{Put94}
\bibcite{Sigaud:2010:MDP:1841781}{SB10}
\bibcite{SuttonBarto}{SB18}
\bibcite{DBLP:journals/pieee/ShahriariSWAF16}{SSW{$^{+}$}16}
\bibcite{wiering2012reinforcement}{WvO12}
\bibstyle{alpha}
